{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11077d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,regularizers, losses\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv3D, Flatten,LSTM,TimeDistributed,BatchNormalization,MaxPooling3D\n",
    "from tensorflow.keras.layers import MaxPooling3D,Dropout,concatenate,AveragePooling2D, Reshape, Add, Conv2D\n",
    "from tensorflow.keras.layers import UpSampling3D, UpSampling2D\n",
    "import random\n",
    "from tensorflow.keras.layers import Conv3DTranspose\n",
    "import open3d as o3d\n",
    "from scipy.io import loadmat\n",
    "import scipy.io\n",
    "# import pyvista as pv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72132563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss_BCE(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def call(self, y_true, y_pred):\n",
    "        \n",
    "        _epsilon = tf.convert_to_tensor(0.000001, y_pred.dtype.base_dtype)\n",
    "        y_pred = tf.clip_by_value(y_pred, _epsilon, 1 - _epsilon)\n",
    "\n",
    "        s1 = tf.math.subtract(1.0, y_true)\n",
    "        s2 = tf.math.subtract(1.0, y_pred)\n",
    "        \n",
    "        a = tf.math.multiply_no_nan(tf.math.log(y_pred), y_true)\n",
    "        b = tf.math.multiply_no_nan(tf.math.log(s2), s1)\n",
    "        out = -(25*a + b)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a38be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Input data\n",
    "\n",
    "# folder_path = '/gpfs-volume/ITU-22/'\n",
    "# c=0\n",
    "# files_names1 = []\n",
    "# for filename in glob.glob(os.path.join(folder_path, 'filesName.mat')):\n",
    "#     files_names1.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40dad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Input data\n",
    "\n",
    "# folder_path = ''\n",
    "# c=0\n",
    "# files_input1 = []\n",
    "# for filename in glob.glob(os.path.join(folder_path, 'pCIR_area2_v2.mat')):\n",
    "#     files_input1.append(filename)\n",
    "\n",
    "# print(len(files_input1))\n",
    "# print(files_input1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0935b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bound=[-16, -16, -2]\n",
    "max_bound=[16,16,2]\n",
    "voxel_size=0.25\n",
    "\n",
    "x_lim = int((max_bound[0] - min_bound[0])/voxel_size);\n",
    "y_lim = int((max_bound[1] - min_bound[1])/voxel_size);\n",
    "z_lim = int((max_bound[2] - min_bound[2])/voxel_size);\n",
    "\n",
    "print(x_lim, y_lim, z_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2(x, y, z):\n",
    "    \n",
    "    x_val = (x - min_bound[0])/voxel_size\n",
    "    y_val = (y - min_bound[1])/voxel_size\n",
    "    z_val = (z - min_bound[2])/voxel_size\n",
    "    \n",
    "    return int(np.floor(x_val)), int(np.floor(y_val)), int(np.floor(z_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mat73\n",
    "# filename_inp = files_input1[0]\n",
    "# with open(filename_inp, 'r') as f:\n",
    "#     rf_signal = mat73.loadmat(f.name)['Pwr_RTP']\n",
    "#     Xp_all_area2= rf_signal\n",
    "# np.save(\"X_all_area3\", Xp_all_area3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This code segment would be needed for training purpose\n",
    "# # For generating the output data in appropriate format\n",
    "\n",
    "# def load_data2(batch_size=1160):\n",
    "    \n",
    "#     Y_all = np.zeros((batch_size, x_lim, y_lim, z_lim))\n",
    "#     aa = batch_size\n",
    "   \n",
    "#     filenamess = files_names1[0]\n",
    "#     with open(filenamess, 'r') as f:\n",
    "#         files = loadmat(f.name)\n",
    "#         #print(files)\n",
    "#         #print(files['filesName'][0])\n",
    "        \n",
    "#     for i in range(batch_size):\n",
    "#         print(i, Y_all[i].shape)\n",
    "#         filename_inp = files['filesName'][i]\n",
    "#         #print(filename_inp)\n",
    "#         filename_out = filename_inp\n",
    "#         filename_out = filename_out.replace('mat', 'pcd')\n",
    "#         filename_out = filename_out.replace('CIR_1', '/gpfs-volume/ITU_challenge_2/area1/lidar/LidarFrame')\n",
    "        \n",
    "#         with open(filename_out, 'r') as f:\n",
    "#             pcd = o3d.io.read_point_cloud(f.name) \n",
    "#             out = np.asarray(pcd.points)\n",
    "            \n",
    "#             s = out.shape[0]\n",
    "#             for ind in range(s):\n",
    "#                 if(out[ind, 0] >= -16 and out[ind, 0] < 16 and out[ind, 1] >= -16 and out[ind, 1] < 16 and out[ind, 2] >= -2 and out[ind, 2] < 2):\n",
    "#                     [x_val, y_val, z_val] = convert(out[ind, :]) \n",
    "#                     Y_all[i, x_val, y_val, z_val] = 1\n",
    "# #             print(np.sum(Y_train[i, :,:,:]))\n",
    "    \n",
    "#    # min_val = np.min(X_all[:])           \n",
    "#    # max_val = np.max(X_all[:])\n",
    "#    # X_all = (X_all - min_val)/(max_val - min_val)\n",
    "    \n",
    "#     return Y_all\n",
    "\n",
    "# Y_all=load_data2()\n",
    "# np.save(\"Y_all_area3\", Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processed input files for area 1 and area 3\n",
    "Xp_all_area1 = np.load(\"X_all_area1.npy\")\n",
    "Xp_all_area3 = np.load(\"X_all_area3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f32aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output LIDAR PCD files converted to voxel grid for training purpose\n",
    "Y_all_area1 = np.load(\"Y_all_area1_0.25.npy\")\n",
    "Y_all_area3 = np.load(\"Y_all_area3_0.25.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.concatenate([Xp_all_area1, Xp_all_area3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = Xp_all_area1[:2400, :, :, :]\n",
    "X_train1 = np.concatenate([X_train1, Xp_all_area3[:1000, :, :, :]])\n",
    "\n",
    "X_test = Xp_all_area1[2400:, :, :, :]\n",
    "X_test = np.concatenate([X_test, Xp_all_area3[1000:, :, :, :]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train1 = Y_all_area1[:2400, :, :, :]\n",
    "Y_train1 = np.concatenate([Y_train1, Y_all_area3[:1000, :, :, :]])\n",
    "\n",
    "Y_test = Y_all_area1[2400:, :, :, :]\n",
    "Y_test = np.concatenate([Y_test, Y_all_area3[1000:, :, :, :]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train1.shape[0]):\n",
    "    min_val = np.min(X_train1[i,:,:,:])           \n",
    "    max_val = np.max(X_train1[i,:,:,:])\n",
    "    X_train1[i,:,:,:] = (X_train1[i,:,:,:]- min_val)/(max_val - min_val)\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    min_val = np.min(X_test[i,:,:,:])           \n",
    "    max_val = np.max(X_test[i,:,:,:])\n",
    "    X_test[i,:,:,:] = (X_test[i,:,:,:]- min_val)/(max_val - min_val)\n",
    "    \n",
    "    \n",
    "# X_train1 = (X_train1 - min_val)/(max_val - min_val)\n",
    "X_train1=np.reshape(X_train1,(3400,6400))\n",
    "\n",
    "# X_test = (X_test - min_val)/(max_val - min_val)\n",
    "X_test=np.reshape(X_test,(347,6400))\n",
    "\n",
    "X_train,X_test1,Y_train,Y_test1=train_test_split(X_train1,Y_train1,test_size=0.00001,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN model-1\n",
    "\n",
    "input_1 = Input(shape = (6400,), name = 'input_1')\n",
    "x = Dense(4096, activation='relu')(input_1)\n",
    "\n",
    "\n",
    "x = tf.reshape(x, [-1,32,32,4,1])\n",
    "\n",
    "\n",
    "x = UpSampling3D((3, 3, 3))(x)\n",
    "\n",
    "\n",
    "x = Conv3D(2,kernel_size = (13,13,2),activation='relu')(x)\n",
    "\n",
    "x = Conv3D(4,kernel_size = (13,13,3),activation='relu')(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv3D(8,kernel_size = (9,9,2),activation='sigmoid')(x)\n",
    "\n",
    "output = tf.reshape(x, [-1,128,128,16])\n",
    "\n",
    "\n",
    "model = Model(inputs=input_1, outputs=output)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74834bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002,decay_steps=10000,decay_rate=0.9)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=opt, loss=CustomLoss_BCE(),metrics=['binary_accuracy'])\n",
    "# #model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iteration  : 28 epochs done\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "for n_epoch in range(epochs):\n",
    "    print(\"Epoch: \", n_epoch)\n",
    "    model.fit(X_train, Y_train, epochs=1, batch_size=batch_size)\n",
    "    model.save_weights('model_norm_cae_Ts_CL_ep100_ArrData_woReg_AA13_epc30.h5')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
